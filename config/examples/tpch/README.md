# TPC-H Example

TPC-H-style order processing example using nomnom, demonstrating parent-child entity relationships with JSON-based data extraction.

## Auto-Generated Project

This project was automatically generated by [nomnom](https://github.com/scie-nz/ingestion/tree/main/nomnom),
a YAML-based code generation framework for data transformation libraries.

## Project Structure

```
tpch/
â”œâ”€â”€ entities/           # YAML entity definitions
â”‚   â”œâ”€â”€ order.yaml      # Root entity (Order)
â”‚   â”œâ”€â”€ orderlineitem.yaml  # Derived entity (OrderLineItem)
â”‚   â”œâ”€â”€ customer.yaml   # Reference entity (Customer)
â”‚   â””â”€â”€ product.yaml    # Reference entity (Product)
â”œâ”€â”€ nomnom.yaml         # Project configuration with custom transforms
â”œâ”€â”€ build.sh            # Build script
â”œâ”€â”€ test.sh             # Test script
â”œâ”€â”€ db.sh               # Database management script
â”œâ”€â”€ docker-compose.yml  # PostgreSQL database setup
â”œâ”€â”€ .env.test           # Database connection settings
â”œâ”€â”€ diesel.toml         # Diesel ORM configuration
â”œâ”€â”€ migrations/         # Database migrations
â””â”€â”€ generate_test_data.py  # Test data generator
```

## Building

Run the build script to generate code and compile the library:

```bash
./build.sh
```

This will:
1. Build the nomnom binary (if needed)
2. Generate Rust code from YAML entity definitions
3. Build the TPC-H library (lib_rust.dylib)
4. Build the record_parser binary

## Testing

Run the test script to verify the parser with random test data:

```bash
# Run all test modes (JSON, SQL, lineage)
./test.sh

# Run with specific parser flags
./test.sh --json-only
./test.sh --show-lineage
./test.sh --json-only --lineage
```

The test script:
- Generates random TPC-H orders using `generate_test_data.py`
- Pipes them through the `record_parser` binary
- Demonstrates all available output modes
- Forwards any command-line flags to the parser

## Database Testing

A throwaway PostgreSQL database is available via Docker Compose for testing SQL generation and data persistence.

### Database Management

```bash
# Start the database
./db.sh start

# Run database migrations
./db.sh migrate

# Check database status
./db.sh status

# Open psql shell
./db.sh shell

# Stop the database
./db.sh stop

# Reset database (removes all data)
./db.sh reset
```

### Database Configuration

- **Host**: localhost
- **Port**: 5433 (avoids conflicts with system PostgreSQL)
- **Database**: tpch_db
- **User**: tpch_user
- **Password**: tpch_password
- **Connection String**: `postgres://tpch_user:tpch_password@localhost:5433/tpch_db`

Connection settings are stored in `.env.test`.

### Running Tests with Database

```bash
# Test with database persistence (generates SQL and loads it into database)
./test.sh --with-db
```

This will:
1. Start the database if not running
2. Generate test data and convert to SQL
3. Execute the SQL against the database
4. Verify data was loaded correctly

### Manual Database Operations

```bash
# Generate SQL from test data
python3 generate_test_data.py --count 10 | ./target/debug/record_parser --sql-only > orders.sql

# Load SQL into database
./db.sh start
./db.sh migrate
docker compose exec -T postgres psql -U tpch_user -d tpch_db < orders.sql

# Query the data
./db.sh shell
```

In psql:
```sql
-- Count orders
SELECT COUNT(*) FROM orders;

-- Count line items
SELECT COUNT(*) FROM order_line_items;

-- View sample data
SELECT * FROM orders LIMIT 5;
SELECT * FROM order_line_items WHERE order_key = 'ORDER-000001';
```

## Entity Model

### Order (Root Entity)
- Loaded from JSON input
- Contains an array of line items (`line_items: List[Object]` field)
- Fields: order_key, customer_key, order_status, total_price, order_date, etc.

### OrderLineItem (Derived Entity)
- Extracted from Order's `line_items` array using `repeated_for` pattern
- Each JSON object in the array becomes an OrderLineItem instance
- Fields: order_key (copied from parent), line_number, part_key, quantity, extended_price, etc.
- Uses custom JSON extraction transforms (json_get_int, json_get_float, json_get_string)

### Customer & Product (Reference Entities)
- Pre-loaded reference data (not processed by parser)
- Would typically be loaded from database or CSV files

## Generating Test Data

The `generate_test_data.py` script generates random TPC-H Order records:

```bash
# Generate 5 orders (default)
python3 generate_test_data.py

# Generate 10 orders
python3 generate_test_data.py --count 10

# Generate with reproducible random seed
python3 generate_test_data.py --count 5 --seed 42

# Pretty-print JSON output
python3 generate_test_data.py --count 2 --pretty
```

## Using the Parser Binary

The `record_parser` binary reads JSON from stdin and outputs entities in various formats:

### JSON Output (JSON Lines format)

```bash
# Parse and output JSON
python3 generate_test_data.py --count 3 | ./target/debug/record_parser --json-only
```

Output:
```json
{"entity_type":"Order","data":{...}}
{"entity_type":"OrderLineItem[0]","data":{...}}
{"entity_type":"OrderLineItem[1]","data":{...}}
```

### SQL Output (Dry-run mode)

```bash
# Generate SQL statements for database insertion
python3 generate_test_data.py --count 2 | ./target/debug/record_parser --sql-only
```

Outputs SQL with:
- SELECT queries to check if entity exists (using unicity fields)
- INSERT queries with properly formatted values (strings quoted, numbers unquoted, JSON arrays)

### Both JSON and SQL (default)

```bash
# Output both JSON and SQL
python3 generate_test_data.py | ./target/debug/record_parser --dry-run
```

### Lineage Tracking

```bash
# Enable lineage metadata in JSON output
python3 generate_test_data.py | ./target/debug/record_parser --json-only --lineage

# Visualize entity lineage tree
python3 generate_test_data.py --count 1 | ./target/debug/record_parser --show-lineage
```

Example lineage tree output:
```
â””â”€ ðŸ’¾ Order [a1b2c3d4]
   â””â”€ ðŸ’¾ OrderLineItem [e5f6g7h8]
   â””â”€ ðŸ’¾ OrderLineItem [i9j0k1l2]
```

## Example Workflow

### Basic Workflow (No Database)

```bash
# 1. Build the project
./build.sh

# 2. Run quick test to verify everything works
./test.sh

# 3. Generate test data and parse to JSON
python3 generate_test_data.py --count 5 --seed 123 \
  | ./target/debug/record_parser --json-only \
  > orders.jsonl

# 4. Generate SQL for database insertion
python3 generate_test_data.py --count 5 --seed 123 \
  | ./target/debug/record_parser --sql-only \
  > orders.sql

# 5. View lineage tree
python3 generate_test_data.py --count 1 \
  | ./target/debug/record_parser --show-lineage
```

### Database Workflow

```bash
# 1. Build the project
./build.sh

# 2. Start database and run migrations
./db.sh start
./db.sh migrate

# 3. Test with database (generates and loads data)
./test.sh --with-db

# 4. Query the data
./db.sh shell
# In psql:
# SELECT * FROM orders;
# SELECT * FROM order_line_items;
# \q

# 5. When done, stop the database
./db.sh stop
```

## Key Features Demonstrated

1. **`repeated_for` Pattern**: OrderLineItem is extracted from Order's JSON array field
2. **Custom Transforms**: JSON extraction functions defined in nomnom.yaml
3. **Type Mapping**: `List[Object]` â†’ `Vec<serde_json::Value>`
4. **Database Persistence**: get_or_create operations with unicity constraints
5. **Type-Aware SQL Generation**: Different SQL helpers for String, numeric, and JSON array types
6. **Parent-Child Relationships**: OrderLineItem automatically includes order_key from parent

## Code Generation

All Rust code is auto-generated from YAML:
- Entity structs with extraction logic (`src/generated.rs`)
- Database models (`src/models/mod.rs`)
- Database schema (`src/schema.rs`)
- Database operations (`src/db/generated_operations.rs`)
- Python bindings (`src/python/generated_persistence.rs`)
- Parser binary with SQL output (`src/bin/record_parser.rs`)

Changes to entity definitions require running `./build.sh` to regenerate code.

## Development

### Modify Entity Definitions

Edit YAML files in `entities/` directory, then rebuild:

```bash
./build.sh
```

### Add Custom Transforms

Add transform definitions to `nomnom.yaml` under the `transforms.rust` section.

### Database Migrations

After modifying entity schemas:

```bash
diesel migration generate name_of_migration
# Edit the generated SQL files
diesel migration run
```

## License

MIT
