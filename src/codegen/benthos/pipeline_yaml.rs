/// Generate Benthos pipeline YAML for a single entity

use crate::codegen::EntityDef;
use super::{BenthosConfig, to_snake_case};
use std::error::Error;

/// Generate Benthos pipeline YAML for an entity
pub fn generate_pipeline_yaml(
    entity: &EntityDef,
    config: &BenthosConfig,
) -> Result<String, Box<dyn Error>> {
    let entity_name_snake = to_snake_case(&entity.name);
    let entity_name = &entity.name;  // Use original CamelCase name for NATS subject

    // Extract field names for column mapping
    let mut columns = Vec::new();
    let mut args_mapping = Vec::new();

    for field in &entity.fields {
        columns.push(field.name.clone());
        args_mapping.push(format!("            this.{}", field.name));
    }

    // Add metadata fields
    columns.push("created_at".to_string());
    args_mapping.push("            now()".to_string());

    let columns_str = columns.iter()
        .map(|c| format!("\"{}\"", c))
        .collect::<Vec<_>>()
        .join(", ");

    let args_mapping_str = args_mapping.join(",\n");

    Ok(format!(r#"# Auto-generated Benthos pipeline for {}
# Generated by nomnom from entity definition
#
# This pipeline:
# - Consumes JSON messages from NATS JetStream
# - Batches messages for efficient MySQL writes
# - Provides connection pooling to minimize overhead
# - ACKs successful writes to trigger NATS cleanup

input:
  nats_jetstream:
    urls:
      - ${{NATS_URL}}
    subject: entities.{}
    durable: {}-mysql-consumer
    deliver: all
    ack_wait: 30s
    max_ack_pending: 1000

pipeline:
  processors:
    # Parse JSON entity
    - bloblang: |
        root = this

    # Add metadata for debugging
    - bloblang: |
        meta nats_subject = @nats_subject
        meta nats_timestamp = @nats_timestamp_unix
        meta processing_time = now()

buffer:
  memory:
    limit: 5000
    batch_policy:
      enabled: true
      count: 100
      period: 1s

output:
  fallback:
    # Primary: Write to MySQL (with ANSI_QUOTES to escape reserved keywords)
    - sql_insert:
        driver: mysql
        dsn: "${{MYSQL_USER}}:${{MYSQL_PASSWORD}}@tcp(${{MYSQL_HOST}}:${{MYSQL_PORT}})/${{MYSQL_DATABASE}}?parseTime=true&sql_mode=ANSI_QUOTES"
        table: "\"{}\""
        columns: [{}]
        args_mapping: |
          root = [
{}
          ]
        conn_max_idle: 20
        conn_max_open: 200
        batching:
          count: 100
          period: 1s

    # Fallback: Send to DLQ on error
    - nats_jetstream:
        urls:
          - ${{NATS_URL}}
        subject: dlq.{}
        max_in_flight: 1

# Prometheus metrics exposed on :4195/metrics
metrics:
  prometheus:
    push_url: ""
    push_interval: ""
    push_job_name: benthos_{}

# JSON structured logging
logger:
  level: ${{LOG_LEVEL:INFO}}
  format: json
  add_timestamp: true
  static_fields:
    '@service': benthos
    '@pipeline': {}
    '@entity': {}

# Health check on :4195/ready and :4195/ping
http:
  address: 0.0.0.0:4195
  enabled: true
  root_path: /benthos
  debug_endpoints: false
"#,
        entity.name,
        entity_name,  // NATS subject uses CamelCase
        entity_name_snake,
        entity_name_snake,
        columns_str,
        args_mapping_str,
        entity_name,  // DLQ subject uses CamelCase
        entity_name_snake,
        entity_name_snake,
        entity.name,
    ))
}
